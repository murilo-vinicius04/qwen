import langchain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferWindowMemory
from langchain_openai import ChatOpenAI
from langchain_core.runnables.history import RunnableWithMessageHistory
import argparse # <<<<<<<<<<<<<<<<< Nova importaÃ§Ã£o

# =================================================================
# ConfiguraÃ§Ã£o do Argumento de Linha de Comando (NOVO!)
# =================================================================
parser = argparse.ArgumentParser(description="Chatbot Qwen com opÃ§Ãµes de debug.")
parser.add_argument(
    "--no-debug", 
    action="store_true", # Se a flag for usada, armazena True
    help="Desativa as mensagens de debug detalhadas da LangChain."
)
args = parser.parse_args()

# Ligue ou desligue o modo debug com base no argumento
if args.no_debug:
    langchain.debug = False
    print("Modo debug da LangChain DESATIVADO. Apenas perguntas e respostas serÃ£o exibidas.\n")
else:
    langchain.debug = True
    print("Modo debug da LangChain ATIVADO. Mensagens detalhadas de execuÃ§Ã£o serÃ£o exibidas.\n")


# =================================================================
# 1. CONFIGURAÃ‡ÃƒO DA CONEXÃƒO (NÃ£o muda)
# =================================================================
llm = ChatOpenAI(
    base_url="http://100.103.176.123:11434/v1",
    api_key="not-needed",
    temperature=0.7,
)

# =================================================================
# 2. O NOVO PROMPT (Estilo Chat)
# =================================================================
# Em vez de um template de string Ãºnica, usamos um ChatPromptTemplate.
# Ele entende a diferenÃ§a entre mensagens do sistema, do histÃ³rico e do usuÃ¡rio.
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            VocÃª Ã© a Lisa, um robÃ´ educacional. Sua missÃ£o Ã© ser o equilÃ­brio perfeito entre um cientista preciso e um professor super gente boa e didÃ¡tico.

            Siga estas regras:
            1.  **Professor Divertido:** Use analogias e metÃ¡foras para explicar temas complexos.
            2.  **Conversa Natural:** Se o usuÃ¡rio usar gÃ­rias ou falar de modo informal, entre na onda! Responda de forma amigÃ¡vel e natural.
            3.  **Confie no Contexto:** Assuma o contexto da conversa. Evite pedir confirmaÃ§Ãµes Ã³bvias.
            4.  **CorreÃ§Ã£o Ã© Prioridade:** Explique a ciÃªncia de forma simples, mas sempre correta.

            ---
            ### Exemplo de DiÃ¡logo Ideal:

            Humano: E aÃ­, Lisa, firmeza? Me explica rapidÃ£o como funciona um motor de carro.

            Lisa: E aÃ­! Firmeza total. Bora lÃ¡! Pensa no motor como uma pipoqueira super potente. VocÃª coloca o 'milho' (que Ã© uma mistura de ar e gasolina), dÃ¡ uma 'faÃ­sca' (a vela de igniÃ§Ã£o), e PÃ! A explosÃ£o empurra uma pecinha (o pistÃ£o) pra baixo com uma forÃ§a bruta. Fazendo isso milhares de vezes por minuto, em vÃ¡rios 'potinhos' diferentes, a gente faz as rodas girarem. Sacou a analogia?
            ---

            Agora, continue a conversa abaixo.

            HistÃ³rico da Conversa:
            {history}

            Humano: {input}
            Lisa:
            """,
        ),
        MessagesPlaceholder(variable_name="history"), # Onde o histÃ³rico serÃ¡ injetado
        ("human", "{input}"), # A nova pergunta do usuÃ¡rio
    ]
)

# =================================================================
# 3. A NOVA CHAIN COM MEMÃ“RIA INTEGRADA
# =================================================================
# Chain principal, sem se preocupar com memÃ³ria ainda.
runnable = prompt | llm

# DicionÃ¡rio para guardar os histÃ³ricos de cada conversa.
# A chave ("abc") Ã© o ID da sessÃ£o. PoderÃ­amos ter vÃ¡rias conversas ao mesmo tempo.
store = {}

def get_session_history(session_id: str):
    """FunÃ§Ã£o que busca o histÃ³rico da sessÃ£o ou cria um novo."""
    if session_id not in store:
        # A memÃ³ria que a gente cria, e a RunnableWithMessageHistory
        # vai interagir com o 'chat_memory' dela.
        store[session_id] = ConversationBufferWindowMemory(k=4, return_messages=True)
    
    # A sacada Ã© retornar o 'chat_memory' que Ã© o objeto ChatMessageHistory de fato
    # que a RunnableWithMessageHistory espera.
    return store[session_id].chat_memory

# A MÃGICA FINAL: Envolvemos nossa chain com o gerenciador de memÃ³ria.
# Ele vai chamar a funÃ§Ã£o get_session_history para carregar/salvar o histÃ³rico.
chain_with_history = RunnableWithMessageHistory(
    runnable,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

# =================================================================
# 4. LOOP PRINCIPAL (Com Streaming!)
# =================================================================
if __name__ == "__main__":
    # Mensagem inicial atualizada para refletir o nome "Lisa"
    print("ðŸ¤– Lisa com memÃ³ria e STREAMING estÃ¡ online! Mande 'sair' pra finalizar.")
    while True:
        pergunta = input("â“ Sua pergunta: ")
        if pergunta.lower() in ["sair", "exit", "q"]:
            break

        print("\nðŸ§  Lisa respondeu:") # Atualizado para Lisa
        resposta_completa = "" # VariÃ¡vel para acumular a resposta completa (se precisar)

        # Usa .stream() em vez de .invoke()
        # Cada 'chunk' Ã© um pedacinho da resposta que vai chegando
        for chunk in chain_with_history.stream(
            {"input": pergunta},
            config={"configurable": {"session_id": "conversa_unica"}}
        ):
            # O chunk jÃ¡ vem como o conteÃºdo da mensagem
            # 'end=""' faz ele nÃ£o pular linha e 'flush=True' garante que imprime na hora
            print(chunk.content, end="", flush=True)
            resposta_completa += chunk.content # Acumula para ter a resposta completa no final

        print("\n") # Pula uma linha no final para organizar o terminal
        # Opcional: Se precisar da resposta completa em uma variÃ¡vel:
        # print(f"DEBUG: Resposta completa acumulada: {resposta_completa}")